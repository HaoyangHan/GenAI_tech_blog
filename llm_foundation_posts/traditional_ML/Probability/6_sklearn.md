---
title: "6 Sklearn"
category: "Traditional ML"
date: "July 27, 2025"
summary: "Essential knowledge on 6 sklearn covering core concepts, algorithms, and hands-on implementation techniques for data science practitioners."
slug: "6-sklearn"
tags: ["data science", "Generative AI", "Math Foundations"]
author: "Haoyang Han"
---



### **Topic: The End-to-End Binary Classification Pipeline**

This guide covers the essential data manipulation and modeling steps to take a raw dataset and produce a trained binary classification model with performance metrics.

### **Part 1: The Core Concept (The "Why" Behind the Pipeline)**

In data science, raw data is rarely, if ever, ready for a machine learning algorithm. Models are essentially mathematical functions that expect clean, numerical, and well-scaled input. The data preparation and modeling pipeline is a systematic, reproducible workflow to transform raw data into this required format and then build and evaluate a model.

**Why does this process matter?**

1.  **Garbage In, Garbage Out:** A model is only as good as the data it's trained on. Missing values, categorical text, and unscaled features can either cause errors or severely degrade model performance.
2.  **Preventing Data Leakage:** The most critical error in machine learning is **data leakage**, where information from the test set inadvertently "leaks" into the training process. This leads to an overly optimistic evaluation, and the model will fail in the real world. Our pipeline structure (especially splitting data *first*) is designed to prevent this.
3.  **Reproducibility & Scalability:** A well-defined pipeline (especially using `sklearn.pipeline.Pipeline`) ensures that every time you run the process, you get the same result. It also makes it easy to apply the exact same transformations to new, incoming data for real-world predictions.

### **Part 2: The Interview Gauntlet (Questions about the Pipeline)**

#### **Conceptual Understanding:**

*   "Walk me through the typical steps you would take from receiving a raw dataset to presenting a final binary classification model."
*   "What is data leakage? Can you give me two examples of how it might happen during preprocessing?"
*   "Why is it critical to split your data into training and testing sets *before* performing imputation or scaling?"
*   "What is the purpose of a `random_state` in functions like `train_test_split` or `RandomForestClassifier`?"

#### **Intuition & Trade-offs:**

*   "When would you choose to impute missing values with the `median` instead of the `mean`?"
*   "You have a categorical feature 'City' with 1,000 unique cities. Why might One-Hot Encoding be a bad idea here? What are some alternatives?"
*   "Explain the difference between `StandardScaler` and `MinMaxScaler`. Which models are sensitive to feature scaling, and which are largely immune?"
*   "Your model has 99% accuracy, but the business says it's useless. What evaluation metric would you investigate first, and why?" (Hint: Class Imbalance).

#### **Troubleshooting & Edge Cases:**

*   "What happens if your test set contains a category in a feature that was not present in the training set? How would you handle this in `scikit-learn`?"
*   "You've built your pipeline, but it's very slow to train. What are the first things you would check or modify?"
*   "Your `RandomForestClassifier` has a much higher performance on the training set than the test set. What is this phenomenon called, and what are three ways you might address it?"

---

### **Part 3 & 4: The Practical Application & Code Challenge**

Here we'll create a mock business case and build the full pipeline step-by-step.

**Mock Case:** A telecom company wants to predict customer churn. You are given a small dataset of customer information and need to build a model to predict whether a customer will churn (`Churn` = 1) or not (`Churn` = 0).

#### **A. The Cheatsheet & The Pipeline**

First, let's import the necessary modules.

```python
# Core libraries
import pandas as pd
import numpy as np

# Data Preprocessing
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score
```

Now, let's create our mock data.

```python
# Create a mock DataFrame for our churn prediction case
data = {
    'CustomerID': range(1, 11),
    'Tenure_Months': [12, 24, 5, 48, 60, 6, 1, 35, 22, 40],
    'Subscription_Type': ['Basic', 'Premium', 'Basic', 'Premium', 'Premium', 'Basic', 'Basic', 'Premium', 'Basic', 'Premium'],
    'Monthly_Bill': [20, 70, 20, 80, 85, np.nan, 15, 75, 25, 78], # Note the missing value
    'Churn': [0, 0, 1, 0, 0, 1, 1, 0, 1, 0] # Target variable
}
df = pd.DataFrame(data)

print("Original Data:")
print(df)
```

#### **Step 1: Train-Test Split**

**Why:** To create a validation set to evaluate our model on unseen data. **CRITICAL:** Do this first!

| Function | Purpose | Key Parameters |
| :--- | :--- | :--- |
| `train_test_split` | Splits arrays or matrices into random train and test subsets. | `test_size`, `random_state`, `stratify` |

*   **Pro Tip:** Use `stratify=y` to ensure the proportion of the target classes is the same in both the train and test sets. This is vital for imbalanced datasets.

```python
# Define features (X) and target (y)
X = df.drop(['CustomerID', 'Churn'], axis=1)
y = df['Churn']

# Split data - STRATIFY is important for classification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
```

#### **Step 2: Define Preprocessing Steps**

We will build a single, powerful preprocessing pipeline using `ColumnTransformer`. This applies different steps to different columns.

*   **Numerical Columns:** `'Tenure_Months'`, `'Monthly_Bill'`
    *   **Action 1: Impute Missing Values.** We'll fill `NaN`s.
    *   **Action 2: Scale.** We'll standardize the features.
*   **Categorical Columns:** `'Subscription_Type'`
    *   **Action 1: Encode.** We'll convert text categories to numerical format.

---
**Cheatsheet: Imputation, Scaling, and Encoding**

| Class | Method | Use Case |
| :--- | :--- | :--- |
| `SimpleImputer` | `strategy='mean'` | Good default for normally distributed data. |
| | `strategy='median'` | Best for data with outliers. |
| `OneHotEncoder` | `handle_unknown='ignore'` | Converts categories to binary columns (0/1). `ignore` prevents errors if test set has new categories. |
| `StandardScaler` | `fit_transform` | Scales data to have a mean of 0 and std dev of 1. Good for most algorithms, including Logistic Regression. |
| `MinMaxScaler` | `fit_transform` | Scales data to a fixed range, usually [0, 1]. Useful for neural networks or when you need bounded values. |

---

```python
# Define which columns are which
numerical_features = ['Tenure_Months', 'Monthly_Bill']
categorical_features = ['Subscription_Type']

# Create the preprocessing pipeline for numerical features
# Step 1: Impute missing values with the median (robust to outliers)
# Step 2: Scale the features
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Create the preprocessing pipeline for categorical features
# Step 1: One-Hot Encode the categories
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps with ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])
```

#### **Step 3: Model Training**

We'll now create a full pipeline that first preprocesses the data and then feeds it into a model. This is best practice.

---
**Cheatsheet: Model Selection**

| Model | Pros | Cons |
| :--- | :--- | :--- |
| `LogisticRegression` | Fast, interpretable, good baseline. | Assumes linearity, may not capture complex patterns. |
| `RandomForestClassifier` | Powerful, handles non-linearity, robust to outliers, less sensitive to scaling. | Less interpretable, can be slow, prone to overfitting if not tuned. |

---
```python
# --- Option 1: Logistic Regression ---
# Create the full pipeline: Preprocessor -> Model
lr_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(random_state=42))
])

# Train the model
lr_pipeline.fit(X_train, y_train)
print("\nLogistic Regression Pipeline Trained!")


# --- Option 2: Random Forest ---
# Create the full pipeline: Preprocessor -> Model
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    # n_estimators: number of trees, max_depth: controls overfitting
    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42))
])

# Train the model
rf_pipeline.fit(X_train, y_train)
print("Random Forest Pipeline Trained!")
```

#### **Step 4: Model Evaluation**

Now, let's use our held-out test set (`X_test`, `y_test`) to see how well our models perform on unseen data.

---
**Cheatsheet: Evaluation Metrics**

| Metric | Function | What it tells you |
| :--- | :--- | :--- |
| **Accuracy** | `accuracy_score` | Overall, what fraction of predictions were correct? (Can be misleading in imbalanced datasets). |
| **Precision** | `classification_report` | Of all positive predictions, how many were actually correct? (Measures False Positives). |
| **Recall** | `classification_report` | Of all actual positives, how many did the model find? (Measures False Negatives). |
| **F1-Score**| `classification_report` | The harmonic mean of Precision and Recall. A good single number to balance the two. |
| **AUC** | `roc_auc_score` | How well the model is at distinguishing between positive and negative classes. An AUC of 1.0 is perfect; 0.5 is a random guess. |

---

```python
# Get predictions from both models
y_pred_lr = lr_pipeline.predict(X_test)
y_pred_rf = rf_pipeline.predict(X_test)

# For AUC, we need prediction probabilities for the positive class (Churn=1)
y_proba_lr = lr_pipeline.predict_proba(X_test)[:, 1]
y_proba_rf = rf_pipeline.predict_proba(X_test)[:, 1]

# --- Evaluate Logistic Regression ---
print("\n--- Logistic Regression Evaluation ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_lr):.2f}")
print(f"AUC Score: {roc_auc_score(y_test, y_proba_lr):.2f}")
print("Classification Report:\n", classification_report(y_test, y_pred_lr))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))

# --- Evaluate Random Forest ---
print("\n--- Random Forest Evaluation ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}")
print(f"AUC Score: {roc_auc_score(y_test, y_proba_rf):.2f}")
print("Classification Report:\n", classification_report(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
```

This complete example provides a robust, reusable, and interview-ready template for any binary classification task. It demonstrates best practices like using `Pipeline` and `ColumnTransformer` to prevent data leakage and ensure reproducibility.


Excellent follow-up questions. These move us from a basic, functional pipeline to a more robust, powerful, and production-ready workflow. Answering these questions well in an interview demonstrates senior-level thinking.

Let's break down these advanced methods.

---

### **1. More Classifiers & Advanced Hyperparameter Tuning**

Manually setting hyperparameters (`n_estimators=100`, `max_depth=5`) is a good start, but the optimal settings depend on the data. The professional approach is to search for them automatically. The most common methods are `GridSearchCV` and `RandomizedSearchCV`.

| Method | How it Works | Pros | Cons |
| :--- | :--- | :--- | :--- |
| **GridSearchCV** | Exhaustively tries every possible combination of the parameters you provide. | Guaranteed to find the best combination within your grid. | Can be extremely slow and computationally expensive if the grid is large. |
| **RandomizedSearchCV** | Samples a fixed number of parameter combinations from a given distribution. | Much faster than Grid Search. Often finds a solution that is nearly as good (or just as good) in a fraction of the time. | Not guaranteed to find the absolute best combination. |

**Pro Tip:** Start with `RandomizedSearchCV` to narrow down the range of good parameters, then you can use `GridSearchCV` on a smaller, more focused search space if needed.

Here's how you'd set up a search for multiple classifiers and their hyperparameters. We'll add **XGBoost**, a powerful gradient boosting model, to our list.

```python
# You might need to install these:
# pip install xgboost
# pip install category_encoders

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, classification_report

# New advanced tools
from category_encoders import TargetEncoder # For advanced encoding
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier # A powerful gradient boosting model
```

### **2. Advanced Methods for Scaler, Split, and Encoder**

Let's enhance our mock data to better showcase the need for these advanced techniques. We'll add outliers and a high-cardinality categorical feature.

```python
# Advanced mock data
data = {
    'Tenure_Months': [1, 5, 6, 12, 22, 24, 35, 40, 48, 60, 90], # Added an outlier (90)
    'Subscription_Type': ['Basic', 'Premium', 'Basic', 'Basic', 'Basic', 'Premium', 'Premium', 'Premium', 'Premium', 'Premium', 'Premium'],
    'City': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'D'], # High cardinality feature
    'Monthly_Bill': [20, 20, 15, 70, 25, 70, 75, 78, 80, 85, 200], # Added an outlier (200)
    'Churn': [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1] # Target variable
}
df = pd.DataFrame(data)

X = df.drop('Churn', axis=1)
y = df['Churn']

# Standard train-test split for final validation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
```

#### **A. Advanced Scalers**

| Scaler | What it does | When to use it |
| :--- | :--- | :--- |
| **`RobustScaler`** | Scales data using statistics that are robust to outliers (the Interquartile Range - IQR). It removes the median and scales by the IQR. | **This is your best choice when your data has significant outliers.** `StandardScaler` would be heavily skewed by the outliers. |
| **`PowerTransformer`** | Applies a power transformation (like Yeo-Johnson or Box-Cox) to make the data more Gaussian-like (normal distribution). | Useful for models that assume normality, like **Logistic Regression**. Can dramatically improve their performance on skewed data. |

#### **B. Advanced Train/Test Split (Evaluation Method: Cross-Validation)**

A single `train_test_split` can be lucky or unlucky. A more robust way to evaluate a model's performance is with **Cross-Validation**.

| Method | What it does | When to use it |
| :--- | :--- | :--- |
| **`StratifiedKFold`** | Splits the data into `k` folds (e.g., 5 or 10). It trains the model on `k-1` folds and tests it on the 1 held-out fold. This is repeated `k` times. The final score is the average of the scores from each fold. `Stratified` ensures the class proportions are maintained in each fold. | **This should be your default method for model evaluation and hyperparameter tuning.** It gives a much more stable and reliable estimate of model performance than a single split. |

#### **C. Advanced Encoder**

When a categorical feature has many unique values (high cardinality), `OneHotEncoder` creates too many new columns, which can hurt model performance (the "curse of dimensionality").

| Encoder | What it does | When to use it |
| :--- | :--- | :--- |
| **`TargetEncoder`** | Replaces each category with the mean of the target variable for that category. For example, if City 'A' has an average churn rate of 0.6, all 'A's are replaced with 0.6. | **Excellent for high-cardinality features.** It captures information about the target variable directly in one feature. **WARNING:** Prone to data leakage/overfitting. Must be used carefully within a cross-validation pipeline to prevent it from seeing the target values of the data it will be scored on. The `category_encoders` library handles this correctly. |

---

### **Putting It All Together: The Advanced Pipeline**

Here is a complete example demonstrating these advanced techniques. We will use `RandomizedSearchCV` with `StratifiedKFold` cross-validation to find the best model and hyperparameters.

#### **Step 1: Define the Advanced Preprocessing Pipeline**

```python
# Identify column types
# Note: 'City' is now a high-cardinality feature
numerical_features = ['Tenure_Months', 'Monthly_Bill']
low_cardinality_categorical = ['Subscription_Type']
high_cardinality_categorical = ['City']

# Create a more advanced preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        # Use RobustScaler for numerical features to handle our new outliers
        ('num', RobustScaler(), numerical_features),
        
        # OneHotEncoder is still fine for low-cardinality features
        ('cat_low', OneHotEncoder(handle_unknown='ignore'), low_cardinality_categorical),
        
        # Use TargetEncoder for the high-cardinality 'City' feature
        ('cat_high', TargetEncoder(), high_cardinality_categorical)
    ],
    remainder='passthrough' # Keep other columns if any; good practice
)
```

#### **Step 2: Set Up the Hyperparameter Search**

```python
# Create the full pipeline with a placeholder for the classifier
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    # We will replace 'classifier' with different models
    ('classifier', RandomForestClassifier()) 
])

# Define the parameter space for each model we want to test
param_grid = [
    # Grid for Logistic Regression (note the 'classifier__' prefix)
    {
        'classifier': [LogisticRegression(solver='liblinear', random_state=42)],
        'classifier__penalty': ['l1', 'l2'],
        'classifier__C': np.logspace(-4, 4, 10), # Search a range of regularization strengths
        'preprocessor__num': [RobustScaler(), StandardScaler(), PowerTransformer()] # Also search different scalers!
    },
    # Grid for Random Forest
    {
        'classifier': [RandomForestClassifier(random_state=42)],
        'classifier__n_estimators': [100, 200, 300],
        'classifier__max_depth': [5, 10, 15, None],
        'classifier__min_samples_leaf': [1, 2, 4]
    },
    # Grid for XGBoost
    {
        'classifier': [XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')],
        'classifier__n_estimators': [100, 200],
        'classifier__learning_rate': [0.01, 0.1, 0.2],
        'classifier__max_depth': [3, 5, 7]
    }
]

# Set up the cross-validation strategy
cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Set up RandomizedSearchCV
# n_iter: number of parameter settings that are sampled.
# scoring: the metric to optimize for (AUC is great for classification)
random_search = RandomizedSearchCV(
    pipeline, 
    param_distributions=param_grid, 
    n_iter=15, # Try 15 different combinations
    cv=cv_strategy, 
    verbose=1, 
    random_state=42, 
    scoring='roc_auc',
    n_jobs=-1 # Use all available CPU cores
)

# Run the search on the training data
random_search.fit(X_train, y_train)
```

#### **Step 3: Analyze Results and Evaluate the Best Model**

```python
print("\n--- Hyperparameter Search Results ---")
print(f"Best parameters found: {random_search.best_params_}")
print(f"Best cross-validation AUC score: {random_search.best_score_:.4f}")

# The random_search object is now the best model, already trained on the full training data
# We can use it to predict on our hold-out test set
best_model = random_search.best_estimator_

# Evaluate the final, tuned model on the unseen test set
y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)[:, 1]

print("\n--- Final Model Evaluation on Test Set ---")
print(f"Test Set AUC Score: {roc_auc_score(y_test, y_proba):.4f}")
print("Test Set Classification Report:\n", classification_report(y_test, y_pred))
```