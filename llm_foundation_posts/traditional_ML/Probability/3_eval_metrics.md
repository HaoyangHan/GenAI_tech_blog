---
title: "3 Eval Metrics"
category: "Statistical Deep Dive"
date: "July 27, 2025"
summary: "Comprehensive guide to 3 eval metrics including mathematical foundations, statistical theory, and practical applications in data science."
slug: "3-eval-metrics"
tags: ["data science", "Generative AI", "Math Foundations"]
author: "Haoyang Han"
---

Excellent! Let's dive deep into **Regression Evaluation Metrics**. This is a fundamental topic, and mastering the nuances between metrics like MSE and MAE is crucial for building and explaining robust regression models.

First, let's analyze the reasons you provided for choosing MSE over MAE. This is a fantastic starting point.

### Analysis of Your Initial Answers

You've correctly identified the core reasons, but there are some critical nuances an interviewer will probe. Let's refine them:

*   **"MSE function is continuous, while MAE function is discontinuous."**
    *   **Correction:** Both the MSE *loss function* and the MAE *loss function* are **continuous**. However, the **derivative** of the MSE function is continuous, while the **derivative** of the MAE function is not. The MAE's derivative is undefined at zero (where `predicted == actual`). This is the key difference that affects optimization.

*   **"MSE metric is more sensitive to Bias of the error than the MAE metric."**
    *   **Correction:** This statement is slightly imprecise. "Bias" in machine learning usually refers to the model's systematic error (i.e., `E[ŷ - y]`). MSE isn't more sensitive to *bias* per se, but it is dramatically more sensitive to **outliers or large errors**. The squaring term makes a single large error dominate the total loss.

*   **"MSE function is a continuously differentiable function, while MAE function is not, which makes calculations with MSE metric easier to implement."**
    *   **Verdict: Correct!** This is the primary mathematical reason. The smooth, well-defined gradient of MSE makes it ideal for optimization algorithms like Gradient Descent.

*   **"MSE function is a convex function, while MAE function is not..."**
    *   **Correction:** This is a common misconception. **Both MSE and MAE are convex functions.** This is a positive attribute for both, as it means an optimization algorithm is guaranteed to find a global minimum and won't get stuck in local minima.

*   **"MSE metric applies greater penalty for greater errors than MAE metric."**
    *   **Verdict: Correct!** This is the core intuitive reason. An error of 10 is penalized as 100 by MSE, but just 10 by MAE. An error of 100 is penalized as 10,000 by MSE, but just 100 by MAE. The penalty grows quadratically, not linearly.

---

With that analysis done, here is your comprehensive Knowledge Skeleton on Regression Evaluation Metrics.

### **Part 1: The Core Concept (Theoretical Foundations)**

In regression, our goal is to predict a continuous value. After training, we are left with a set of true values (`y`) and our model's predicted values (`ŷ`). The difference between them (`y - ŷ`) is the **error** or **residual**. Regression evaluation metrics are functions that aggregate these individual errors into a single score to quantify the model's performance.

Let `n` be the number of data points, `yᵢ` be the true value for the i-th data point, and `ŷᵢ` be the predicted value.

**1. Mean Absolute Error (MAE)**
*   **What it is:** The average of the absolute differences between predicted and actual values.
*   **Intuition:** "On average, how far off were our predictions?" It's easy to understand because its units are the same as the target variable (e.g., dollars, temperature). It is robust to outliers.
*   **Formula:**
    ```
    MAE = (1/n) * Σ |yᵢ - ŷᵢ|
    ```

**2. Mean Squared Error (MSE)**
*   **What it is:** The average of the squared differences between predicted and actual values.
*   **Intuition:** A metric that heavily penalizes larger errors. Because the errors are squared, its units are the square of the target variable's units (e.g., dollars squared), making it less intuitive for reporting.
*   **Formula:**
    ```
    MSE = (1/n) * Σ (yᵢ - ŷᵢ)²
    ```

**3. Root Mean Squared Error (RMSE)**
*   **What it is:** The square root of the Mean Squared Error.
*   **Intuition:** A popular "compromise" metric. Like MSE, it penalizes large errors, but by taking the square root, it returns the error metric to the original units of the target variable (e.g., dollars). It's often used for reporting model performance. **Minimizing RMSE is the same as minimizing MSE.**
*   **Formula:**
    ```
    RMSE = sqrt( (1/n) * Σ (yᵢ - ŷᵢ)² )
    ```

**4. R-squared (R²) or Coefficient of Determination**
*   **What it is:** The proportion of the variance in the dependent variable that is predictable from the independent variable(s).
*   **Intuition:** A relative measure of "goodness of fit." It answers the question: "How much better is our model than a naive model that just predicts the mean of the target variable?" An R² of 0.75 means the model explains 75% of the variance in the target. It is unitless.
*   **Formula:**
    ```
    R² = 1 - ( Σ(yᵢ - ŷᵢ)² / Σ(yᵢ - ȳ)² )
    ```
    Where `ȳ` is the mean of the true values.

---

### **Part 2: The Interview Gauntlet (Theoretical Questions)**

#### **Conceptual Understanding:**

1.  What is the primary difference in how MAE and MSE handle errors?
2.  Your model predicts house prices in dollars. You calculate an RMSE of 50,000 and an MAE of 35,000. How would you explain what these two numbers mean to a non-technical stakeholder?
3.  Why is RMSE often preferred over MSE for *reporting* model performance?
4.  What are the units of MAE, MSE, RMSE, and R-squared if your target variable is "car price in $"?
5.  What does an R-squared value of 0 mean? What about a value of 1? Is it possible to get a negative R-squared?

#### **Intuition & Trade-offs:**

6.  **(The Classic Outlier Question)** You are predicting customer lifetime value. Your dataset has a few "whale" customers with extremely high values (outliers). If you train a model using MSE as the loss function, what might happen? What if you used MAE?
7.  You have two models. Model A has a lower MAE, but Model B has a lower RMSE. Which model would you choose and why? What does this situation tell you about the error distributions of the two models?
8.  Why is simply choosing the model with the highest R-squared not always the best strategy?
9.  If your goal is to build a model where large errors are exceptionally undesirable (e.g., predicting stress on a critical bridge support beam), which metric would you prioritize minimizing: MAE or MSE/RMSE? Why?

#### **Troubleshooting & Edge Cases:**

10. You built a model and got a very low RMSE, but also a very low R-squared (e.g., 0.15). What could this indicate about your target variable?
11. Why is minimizing MSE during optimization mathematically equivalent to minimizing RMSE?
12. Most standard linear regression libraries (like `scikit-learn`'s `LinearRegression`) solve for the model coefficients by minimizing MSE. Why don't they offer an option to minimize MAE by default?

---

### **Part 3: The Practical Application (Code & Implementation)**

In a typical `scikit-learn` workflow, you use these metrics *after* you have trained a model and made predictions. The core functions reside in the `sklearn.metrics` module.

1.  **Split your data:** You start with `X_train`, `X_test`, `y_train`, `y_test`.
2.  **Train your model:** `model.fit(X_train, y_train)`
3.  **Make predictions:** `y_pred = model.predict(X_test)`
4.  **Evaluate:** You then compare `y_test` (the ground truth) with `y_pred` (your model's predictions) using the metrics.

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assume you have features X and a target y
# X, y = ...

# 1. Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Train model
model = LinearRegression()
model.fit(X_train, y_train)

# 3. Make predictions
y_pred = model.predict(X_test)

# 4. Evaluate using sklearn.metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse) # Or mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R²): {r2:.2f}")
```

It's important to note that while you use these for *evaluation*, many models implicitly use one for *training*. Standard `LinearRegression` finds the coefficients that **minimize MSE**. Some models, like `SGDRegressor`, allow you to specify the loss function, giving you more direct control.

---

### **Part 4: The Code Challenge (Practical Questions)**

**Scenario:**
You work for a car dealership and have built two different models to predict the price of used cars. You have the true prices and the predictions from both models for five cars.

**Task:**
1.  Using `pandas` and `scikit-learn`, calculate the MAE, MSE, RMSE, and R-squared for both Model A and Model B.
2.  Based on your calculations, write a one-paragraph summary explaining which model you would recommend and why, specifically discussing the trade-offs revealed by the different metrics.

**Data:**

```python
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

data = {
    'true_price': [25000, 48000, 18000, 65000, 32000],
    'model_A_pred': [26000, 47000, 19500, 63000, 33000],
    'model_B_pred': [25500, 47500, 18500, 75000, 31500]
}
df = pd.DataFrame(data)

y_true = df['true_price']
y_pred_A = df['model_A_pred']
y_pred_B = df['model_B_pred']
```

---
### **Code Challenge Answer**

**1. Python Code for Calculation**

```python
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Data Setup
data = {
    'true_price': [25000, 48000, 18000, 65000, 32000],
    'model_A_pred': [26000, 47000, 19500, 63000, 33000],
    'model_B_pred': [25500, 47500, 18500, 75000, 31500]
}
df = pd.DataFrame(data)

y_true = df['true_price']
y_pred_A = df['model_A_pred']
y_pred_B = df['model_B_pred']

# --- Calculate Metrics for Model A ---
mae_A = mean_absolute_error(y_true, y_pred_A)
mse_A = mean_squared_error(y_true, y_pred_A)
rmse_A = np.sqrt(mse_A)
r2_A = r2_score(y_true, y_pred_A)

# --- Calculate Metrics for Model B ---
mae_B = mean_absolute_error(y_true, y_pred_B)
mse_B = mean_squared_error(y_true, y_pred_B)
rmse_B = np.sqrt(mse_B)
r2_B = r2_score(y_true, y_pred_B)

# --- Print Results ---
print("--- Model A ---")
print(f"MAE:  ${mae_A:,.2f}")
print(f"RMSE: ${rmse_A:,.2f}")
print(f"R²:   {r2_A:.4f}\n")

print("--- Model B ---")
print(f"MAE:  ${mae_B:,.2f}")
print(f"RMSE: ${rmse_B:,.2f}")
print(f"R²:   {r2_B:.4f}")

# For context, let's also look at the underlying MSE
# print(f"\nMSE for Model A: {mse_A:,.2f}")
# print(f"MSE for Model B: {mse_B:,.2f}")
```

**Expected Output:**

```
--- Model A ---
MAE:  $1,300.00
RMSE: $1,442.22
R²:   0.9881

--- Model B ---
MAE:  $2,400.00
RMSE: $4,636.81
R²:   0.8770
```

**2. Recommendation Summary**

Based on the metrics, **I would recommend Model A.** Here's the justification: Model A has a significantly lower MAE (\~$1,300) and RMSE (\~$1,440) compared to Model B (MAE: \$2,400, RMSE: \$4,630). This indicates that Model A's predictions are consistently closer to the true prices. The key insight comes from comparing the MAE and RMSE for Model B. Its RMSE is nearly double its MAE, which suggests that Model B makes some very large errors. Looking at the data, we can see Model B was off by \$10,000 on the \$65,000 car. The RMSE, due to its squaring nature, heavily penalizes this single large error, causing its score to be much worse. While Model B is good on average for some predictions, its unreliability and tendency for huge mistakes make Model A the safer and more accurate choice for the dealership.