---
title: "2 Imbalanced Data"
category: "Statistical Deep Dive"
date: "July 27, 2025"
summary: "Data science foundations covering 2 imbalanced data with theoretical insights and practical applications."
slug: "2-imbalanced-data"
tags: ["data science", "Generative AI", "Math Foundations"]
author: "Haoyang Han"
---

### The Question and Solution

**Problem:** You are working on an image classification problem to predict whether an image contains a specific object of interest, with a majority of images not containing the object. There is a significant class imbalance between images with and without the object.

**Instruction:** Choose valid approaches to handle data imbalance in this scenario.

**Analysis of Options:**

1.  ✅ **Use data augmentation techniques such as rotation, scaling, and flipping to generate new examples of images containing the object of interest.**
    *   **Verdict: VALID.** This is a cornerstone technique. It's a form of oversampling where you create new, modified copies of your minority class images. This helps the model learn the essential features of the object under various conditions, reducing overfitting and improving generalization for the rare class.

2.  ✅ **Use image segmentation techniques to identify regions of interest in the minority class and oversample these regions to create new training examples.**
    *   **Verdict: VALID.** This is a sophisticated and effective form of synthetic data generation. By segmenting the object, you can crop it and paste it onto various backgrounds, creating highly realistic new samples for the minority class. This can be more powerful than simple geometric augmentations.

3.  ❌ **Perform clustering on the minority class to identify highly similar instances and remove images that are difficult to classify or have noisy labels.**
    *   **Verdict: INVALID (as a primary imbalance strategy).** This describes a data cleaning or under-sampling technique. Removing data points from an *already small* minority class is generally counterproductive to solving the imbalance problem. While cleaning noisy labels is good practice, this action *worsens* the class imbalance rather than correcting it.

4.  ✅ **Oversample the minority class by merging two objects into one using techniques such as image blending.**
    *   **Verdict: VALID.** This refers to advanced augmentation techniques like **Mixup** or **CutMix**. Mixup, for example, creates new samples by taking a weighted linear combination of two images and their labels. Applying this to minority class images is a powerful regularization method that helps the model learn robust features and handle data scarcity.

5.  ✅ **Apply transfer learning with pre-trained models on balanced datasets to leverage pre-existing feature representations without being affected by the imbalance in the target dataset.**
    *   **Verdict: VALID.** Transfer learning is a highly effective strategy. A model pre-trained on a massive, balanced dataset (like ImageNet) has already learned a rich hierarchy of visual features. When you fine-tune this model, its powerful feature extractor makes it much more data-efficient. It can learn to distinguish your specific classes with far fewer examples, making it inherently more robust to the class imbalance in your smaller, specialized dataset.



### **Enhanced Knowledge Skeleton: Handling Class Imbalance**

---

### **Part 1: The Core Concept (Deepened Theoretical Foundations)**

**What is Class Imbalance?**
Class imbalance is a characteristic of a dataset where the number of observations per class is not equally distributed. While the previous definition holds, it's crucial to understand the *nuance*:

*   **Intrinsic vs. Extrinsic Imbalance:** Imbalance can be *intrinsic* to the problem domain (e.g., fraudulent transactions are naturally rare) or *extrinsic* due to data collection methods (e.g., a faulty sensor leads to under-reporting of a certain event).
*   **The Problem is About Information, Not Just Numbers:** The core issue isn't the ratio itself, but the lack of *sufficient information* for the model to learn the decision boundary for the minority class. A 1:100 ratio is not a problem if the two classes are perfectly separable (e.g., all minority class points are clustered in one corner of the feature space). The problem becomes severe when the minority class is rare *and* its instances are complex, varied, or overlap with the majority class.

**Why Does It Matter? (Revisited)**
Standard loss functions, like Cross-Entropy, are sums over all samples. With a 99:1 imbalance, 99% of the loss (and thus the gradients that update the model's weights) comes from the majority class. The model's learning is therefore dominated by the majority class, and it has little incentive to correctly classify the few minority samples.

**Core Strategies (Expanded & Modernized):**

1.  **Data-Level Approaches:**
    *   **Under-sampling:** Removing majority class samples.
        *   *Simple:* Random Under-sampling.
        *   *Intelligent:* Use algorithms like **Tomek Links** (remove majority-class members of a pair of nearest neighbors from different classes) or **Edited Nearest Neighbors (ENN)** to clean the boundary between classes by removing majority samples that are misclassified by their neighbors. These are "cleaning" techniques that make the decision boundary less noisy.
    *   **Oversampling:** Increasing minority class samples.
        *   *Simple:* Random Over-sampling.
        *   *Intelligent (Synthetic Data Generation):*
            *   **SMOTE (Synthetic Minority Over-sampling TEchnique):** Creates new samples by interpolating between a minority sample and one of its *k*-nearest minority neighbors.
            *   **ADASYN (Adaptive Synthetic Sampling):** A variant of SMOTE that generates more synthetic data for minority class examples that are *harder to learn* (i.e., those with more majority class neighbors). It adaptively shifts the decision boundary to focus on difficult examples.
    *   **Advanced Data Augmentation (for Images/Unstructured Data):**
        *   **Mixup:** Creates new training examples by taking a convex combination of pairs of images and their labels. For `(image1, label1)` and `(image2, label2)`, a new sample is `(λ*image1 + (1-λ)*image2, λ*label1 + (1-λ)*label2)`. This acts as a powerful regularizer and forces the model to learn smoother, more robust decision boundaries.
        *   **CutMix:** Another regularization technique where a patch from one image is cut and pasted onto another, and the label is mixed proportionally to the area of the patch. It encourages the model to learn from local regions rather than relying on the entire image context.

2.  **Algorithm-Level (Cost-Sensitive) Approaches:**
    *   **Class Weighting:** The classic approach of assigning higher weights to the minority class in the loss function. It's a direct way to tell the model, "getting this sample wrong is X times worse than getting a majority sample wrong."
    *   **Focal Loss:** A revolutionary loss function introduced for object detection where imbalance between foreground (object) and background is extreme. It dynamically scales the cross-entropy loss. It down-weights the loss assigned to well-classified examples, forcing the model to focus its training effort on the "hard" examples that it consistently gets wrong. This is often the minority class. The formula is: `FL(pt) = -α * (1 - pt)^γ * log(pt)`, where `pt` is the model's predicted probability for the ground-truth class, `γ` (gamma) is the focusing parameter, and `α` (alpha) is a weighting factor.

3.  **Evaluation-Level Approaches (Re-emphasized):**
    *   **Precision-Recall Curve (PR Curve):** For severe imbalance, the PR curve is often more insightful than the ROC curve. The ROC curve's False Positive Rate can be misleadingly low when the number of true negatives is massive. The PR curve plots Precision vs. Recall and better highlights the performance on the small positive class. The **Average Precision (AP)** or **AUC-PR** is the key metric here.

---

### **Part 2: The Interview Gauntlet (Deepened Theoretical Questions)**

#### **Conceptual Understanding**

*   (Unchanged) Why is accuracy a misleading metric for imbalanced data?
*   What is the difference between ADASYN and SMOTE? When would you prefer one over the other?
    *   *Hint:* ADASYN focuses on generating samples near the decision boundary, which can be good but also risks creating noisy samples. SMOTE is more uniform. Choose ADASYN when you believe the boundary is complex and needs reinforcement.

#### **Intuition & Trade-offs**

*   What are the main risks of using SMOTE? How can you mitigate them?
    *   *Hint:* SMOTE can create noisy samples by interpolating between outliers or in regions of class overlap. It can also blur the decision boundary. Mitigation includes combining SMOTE with an under-sampling cleaning method like Tomek Links (often called SMOTE-Tomek).
*   When might you choose an algorithm-level approach (like Focal Loss) over a data-level approach (like SMOTE)?
    *   *Hint:* For very large datasets (especially images), creating a resampled version can be computationally expensive and consume huge amounts of disk space. Algorithm-level approaches work on-the-fly without altering the dataset itself. They also often provide better regularization (like Focal Loss focusing on hard examples).

#### **Advanced & Modern Techniques**

*   **Explain Focal Loss.** What do the `gamma` and `alpha` parameters control, and how does it help with class imbalance?
    *   *Hint:* `gamma` (the focusing parameter) controls the rate at which easy examples are down-weighted. A higher `gamma` means more focus on hard examples. `alpha` is a static weighting factor, similar to `class_weight`, that directly balances the importance of positive/negative classes.
*   **How does Mixup or CutMix help with class imbalance?** Isn't it just a regularization technique?
    *   *Hint:* It's both. By creating synthetic examples, it expands the training distribution, similar to oversampling. When mixing a minority sample with a majority sample, it creates a new "in-between" example that teaches the model to be less confident at the class boundaries, making it more robust to the minority class.
*   You've trained a model using `class_weight='balanced'`. You notice that your Recall is now excellent (95%), but your Precision has dropped to 20%. What has happened, and what is your next step?
    *   *Hint:* The model has become too aggressive in predicting the positive class to avoid the high penalty for missing them (false negatives). It is now over-predicting the minority class, leading to many false positives. The next step is to tune the class weight manually (e.g., instead of a 1:20 ratio, try 1:10 or 1:5) or to adjust the model's prediction threshold post-training to find a better balance between precision and recall on the validation set.

---

### **Part 3: The Practical Application (Expanded Code & Implementation)**

This section now includes implementations of more advanced techniques.

**Focal Loss Implementation in PyTorch**
Since Focal Loss isn't a standard part of `torch.nn`, you often have to implement it yourself in an interview.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        # inputs are raw logits, targets are class indices
        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)  # calculates the probability of the true class
        F_loss = self.alpha * (1 - pt)**self.gamma * BCE_loss

        if self.reduction == 'mean':
            return torch.mean(F_loss)
        elif self.reduction == 'sum':
            return torch.sum(F_loss)
        else:
            return F_loss

# Usage in a training loop:
# criterion = FocalLoss(alpha=0.5, gamma=2.0)
# loss = criterion(outputs, labels)
# loss.backward()
```

**Mixup/CutMix Logic**
While full implementation can be complex, describing the logic is key. It's applied *inside* the training loop, per batch.

```python
# Conceptual logic for Mixup in a training loop
# (Libraries like 'timm' have this built-in)

for images, labels in train_loader:
    # 1. Generate a mixing coefficient lambda
    lam = np.random.beta(alpha, alpha) # alpha is a hyperparameter, often 1.0

    # 2. Shuffle the batch to get mixed pairs
    batch_size = images.size(0)
    shuffled_indices = torch.randperm(batch_size)
    
    shuffled_images = images[shuffled_indices, :]
    shuffled_labels = labels[shuffled_indices]

    # 3. Create the mixed batch
    mixed_images = lam * images + (1 - lam) * shuffled_images
    
    # One-hot encode labels for mixing
    labels_onehot = F.one_hot(labels, num_classes=num_classes).float()
    shuffled_labels_onehot = F.one_hot(shuffled_labels, num_classes=num_classes).float()
    mixed_labels = lam * labels_onehot + (1 - lam) * shuffled_labels_onehot
    
    # 4. Proceed with training
    optimizer.zero_grad()
    outputs = model(mixed_images)
    
    # Special loss function for mixed labels (e.g., softened cross-entropy)
    loss = loss_function(outputs, mixed_labels) 
    loss.backward()
    optimizer.step()
```

---

### **Part 4: The Code Challenge (New Advanced Challenge)**

**The Task:**
You are working on the same tumor detection problem. You have already implemented a `WeightedRandomSampler` to create balanced batches, but you find your model's performance is still limited. You hypothesize that the model is struggling because some "no tumor" images look very similar to "tumor" images (i.e., they are "hard negatives"), while many are very easy (e.g., blank scans).

Your task is to **write the Python code for a `FocalLoss` class in PyTorch** and explain how you would integrate it into your training pipeline to solve this "hard example" problem.

**Answer (Code & Explanation):**

"To address the problem of hard negatives, I would replace the standard `nn.CrossEntropyLoss` with a custom `FocalLoss` implementation. Focal Loss will force the model to focus more on these difficult-to-classify examples by reducing the loss contribution from easy examples (both positive and negative), which would otherwise dominate the training.

Here is the PyTorch implementation I would use:"

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    """
    Implements the Focal Loss from the paper 'Focal Loss for Dense Object Detection'.
    It's designed to address class imbalance by down-weighting easy examples.
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        """
        Args:
            alpha (float): The weighting factor for the positive class. Helps address
                           the raw class imbalance. A common value is 0.25.
            gamma (float): The focusing parameter. A higher gamma (>0) reduces the
                           relative loss for well-classified examples (pt -> 1),
                           putting more focus on hard, misclassified examples.
            reduction (str): 'mean', 'sum', or 'none'.
        """
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        """
        Args:
            inputs (torch.Tensor): The model's raw output logits (shape: [N, C]).
            targets (torch.Tensor): The ground truth labels (shape: [N]).
        """
        # Calculate the standard cross entropy loss but without reduction
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')

        # Calculate the probability of the correct class
        # pt = e^(-ce_loss) is a numerically stable way to get the probability
        pt = torch.exp(-ce_loss)

        # This is the core of Focal Loss
        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss
        
        # Apply the specified reduction
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

```

**Integration into the Training Pipeline:**

"To integrate this, I would simply replace the line where I define my criterion in the training script:

**From:**
`criterion = nn.CrossEntropyLoss()`

**To:**
`criterion = FocalLoss(alpha=0.25, gamma=2.0)`

I would then treat `alpha` and `gamma` as hyperparameters to be tuned on my validation set. A `gamma` of 2 is a common starting point. A higher `alpha` can be used if I still need to give more static weight to the positive class. This change directly targets the learning algorithm to handle the imbalance of *example difficulty*, which complements the data-level balancing from the `WeightedRandomSampler`."